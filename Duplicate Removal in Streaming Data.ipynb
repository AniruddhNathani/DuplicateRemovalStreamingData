{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Removal in Streaming Data\n",
    "## Submitted By - Aniruddh Nathani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1) Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Randomly generated Real Time Data is: \n",
      " {'1am': 8, '2am': 8, '3am': 2, '4am': 8, '5am': 8, '6am': 9, '7am': 4, '8am': 1, '9am': 2, '10am': 9}\n",
      "\n",
      "############### Duplicate removal in Progress ###################\n",
      "\n",
      "Print mapping at Time Stamp 1am.\n",
      "{'1am': 8} \n",
      "\n",
      "Data 8 at Time 2am is already seen at a previous Time Stamp. Hence Ignore\n",
      "\n",
      "Print mapping at Time Stamp 3am.\n",
      "{'1am': 8, '3am': 2} \n",
      "\n",
      "Data 8 at Time 4am is already seen at a previous Time Stamp. Hence Ignore\n",
      "\n",
      "Data 8 at Time 5am is already seen at a previous Time Stamp. Hence Ignore\n",
      "\n",
      "Print mapping at Time Stamp 6am.\n",
      "{'1am': 8, '3am': 2, '6am': 9} \n",
      "\n",
      "Print mapping at Time Stamp 7am.\n",
      "{'1am': 8, '3am': 2, '6am': 9, '7am': 4} \n",
      "\n",
      "Print mapping at Time Stamp 8am.\n",
      "{'1am': 8, '3am': 2, '6am': 9, '7am': 4, '8am': 1} \n",
      "\n",
      "Data 2 at Time 9am is already seen at a previous Time Stamp. Hence Ignore\n",
      "\n",
      "Data 9 at Time 10am is already seen at a previous Time Stamp. Hence Ignore\n",
      "\n",
      "############### Duplicate removal Complete ###################\n",
      "\n",
      "\n",
      "Data After Removing Duplicates: \n",
      " {'1am': 8, '3am': 2, '6am': 9, '7am': 4, '8am': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from random import randint\n",
    "\n",
    "# Function which checks duplicate data present in the Real time Data or not. The function  can be connected to\n",
    "#any realtime data generator API which gives data in the form of JSON Objects.\n",
    "def DuplicateChecker(key, value, mapping):\n",
    "    if not mapping:\n",
    "        print(\"Print mapping at Time Stamp {}.\".format(key))\n",
    "        mapping[key] = value\n",
    "    else:\n",
    "        if value in mapping.values():\n",
    "            print(\"Data {} at Time {}\".format(value, key), \"is already seen at a previous Time Stamp.\", \"Hence Ignore\\n\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Print mapping at Time Stamp {}.\".format(key))\n",
    "            mapping[key] = value\n",
    "    print(dict(mapping),\"\\n\")\n",
    "    return mapping\n",
    "    \n",
    "\n",
    "### Let real Time Data be in the JSON form. Assuming this data is randomly generated  for now, \n",
    "### the function created above can be called and used for streaming data as well.\n",
    "\n",
    "\n",
    "RealTimeData = {}\n",
    "for i in range(1, 11):\n",
    "    RealTimeData[str(i) + 'am'] = randint(1, 10)  \n",
    "print(\"Original Randomly generated Real Time Data is: \\n\" ,RealTimeData)\n",
    "\n",
    "print(\"\\n############### Duplicate removal in Progress ###################\\n\")\n",
    "\n",
    "#creating hash map which keeps only unduplicated values.\n",
    "mapping = defaultdict(lambda : 0) \n",
    "\n",
    "#for loop for Real time data seen at each timestamp. .\n",
    "for key,value in RealTimeData.items():\n",
    "    DuplicateChecker(key, value, mapping)\n",
    "   \n",
    "print(\"############### Duplicate removal Complete ###################\\n\")\n",
    "print(\"\\nData After Removing Duplicates: \\n\",dict(mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2) Deduplication Solution in-stream data using Spark  (When data is huge)\n",
    "#### 1)Spark flow that filters events based on their presence in a lookup database (cache storage)\n",
    "#### 2) and Amazon S3 for storing Duplicator Caches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Steps:\n",
    "1) Partition Data into a Spark Stream. Each event has a unique ID based on the creation Time Stamp. Place in a bucket whose number is calculated as a hashcode of the event ID.\n",
    "\n",
    "2) For better performance, Run the deduplication processing in parallel. \n",
    "\n",
    "3) Since we already processed data in parallel with the partitioning scheme described in 1), we should reuse the same parallelism for deduplication. Therefore, we should partition the cache stored in the S3 filesystem the same way we partitioned data in the stream.\n",
    "\n",
    "4) Cache is a simple Data Structure that consists of two \"SETS\". The first one was an immutable set which contained data loaded from S3. The second one was a mutable set which was used for adding new event IDs.\n",
    "\n",
    "5) Since set stores unique elements, On deduplication processing we get all the data elements which have unique data values and the ones which are repeated are ignored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
